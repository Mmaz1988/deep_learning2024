{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Deep Learning with Python</h1>\n",
    "<h2>Working with the server and the GPU</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for any other library, keras must be installed on your machine. If you are also working on your local machine, this is the right time to install it. However, this procedure takes away quite some space from your home directory on the server. For this reason, we have installed it for you on Hulk.\n",
    "\n",
    "Let's now open a new **terminal** and set this up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can I run code on the GPU?\n",
    "\n",
    "To run code on the GPU you not only must have a version of Keras (or Pytorch) that allows you that, but you also must have the right permissions. In the terminal you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should all have listed:\n",
    "    \n",
    "    * cl-students\n",
    "    * gpu \n",
    "    * video\n",
    "\n",
    "Without the last 2 groups, you would not be able to run code on the GPU. This is specific to our server, not a general condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How busy is the server?\n",
    "\n",
    "With so many users working on the same machine, it can happen that the server is too busy. In this case, your computation would take longer than usual to finish. In the terminal, use `htop` to get an overview of the GPU/ram usage on Hulk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this code in the terminal \n",
    "\n",
    "!htop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this code in the terminal \n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to work with the server and activate GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the GPU and push into Torch\n",
    "\n",
    "The GPU is being selected automatically using PyTorch's built-in functions.\n",
    "\n",
    "In this case there is no need to set os.environ[\"CUDA_VISIBLE_DEVICES\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available(): #check if GPU is available      \n",
    "    device = torch.device(\"cuda:1\") #set device, we need this later to push our model and the data to the GPU to perform computations\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking availability\n",
    "!nvidia-smi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the GPU index you want to use manually\n",
    "\n",
    "# import torch\n",
    "\n",
    "selected_gpu = 0  # Change this to 0, 1, or 2 depending on which GPU you want to use\n",
    "\n",
    "# Check if the specified GPU is available\n",
    "if torch.cuda.is_available() and selected_gpu < torch.cuda.device_count():\n",
    "    torch.cuda.set_device(selected_gpu)  # Set the selected GPU as the current device\n",
    "    device = torch.device(f\"cuda:{selected_gpu}\")\n",
    "    print(f'Using GPU {selected_gpu}.')\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {i} name:', torch.cuda.get_device_name(i))\n",
    "    print('Current GPU:', torch.cuda.current_device())\n",
    "    \n",
    "else:\n",
    "    print('No specified GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
